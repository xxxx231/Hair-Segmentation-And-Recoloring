{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1F3ThgFrekv0ZjF2NBxBLdDF4ti6EYccB",
      "authorship_tag": "ABX9TyP5KxLEnhVTCpDFTAVeDCMz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xxxx231/Hair-Segmentation-And-Recoloring/blob/main/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h34a3VDyg175"
      },
      "source": [
        "'''Code được tham khảo từ RIS AI HairNet Hair & Head Segmentation'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKPu9E22bF7C"
      },
      "source": [
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
        "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers.merge import concatenate, add\n",
        "\n",
        "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
        "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
        "    # first layer\n",
        "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    # second layer\n",
        "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
        "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "\n",
        "def get_unet(input_img, n_filters = 16, dropout = 0.1, batchnorm = True):\n",
        "    \"\"\"Function to define the UNET Model\"\"\"\n",
        "    # Contracting Path\n",
        "    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "    p1 = Dropout(dropout)(p1)\n",
        "    \n",
        "    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "    p2 = Dropout(dropout)(p2)\n",
        "    \n",
        "    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
        "    p3 = MaxPooling2D((2, 2))(c3)\n",
        "    p3 = Dropout(dropout)(p3)\n",
        "    \n",
        "    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
        "    p4 = MaxPooling2D((2, 2))(c4)\n",
        "    p4 = Dropout(dropout)(p4)\n",
        "    \n",
        "    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n",
        "    p5 = MaxPooling2D((2, 2))(c5)\n",
        "    p5 = Dropout(dropout)(p5)\n",
        "\n",
        "    c6 = conv2d_block(p5, n_filters = n_filters * 32, kernel_size = 3, batchnorm = batchnorm)\n",
        "    # Expansive Path\n",
        "    u7 = Conv2DTranspose(n_filters * 16, (3, 3), strides = (2, 2), padding = 'same')(c6)\n",
        "    u7 = concatenate([u7, c5])\n",
        "    u7 = Dropout(dropout)(u7)\n",
        "    c7 = conv2d_block(u7, n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n",
        "    \n",
        "    u8 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c7)\n",
        "    u8 = concatenate([u8, c4])\n",
        "    u8 = Dropout(dropout)(u8)\n",
        "    c8 = conv2d_block(u8, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
        "    \n",
        "    u9 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c8)\n",
        "    u9 = concatenate([u9, c3])\n",
        "    u9 = Dropout(dropout)(u9)\n",
        "    c9 = conv2d_block(u9, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
        "    \n",
        "    u10 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c9)\n",
        "    u10 = concatenate([u10, c2])\n",
        "    u10 = Dropout(dropout)(u10)\n",
        "    c10 = conv2d_block(u10, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
        "\n",
        "    u11 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c10)\n",
        "    u11 = concatenate([u11, c1])\n",
        "    u11 = Dropout(dropout)(u11)\n",
        "    c11 = conv2d_block(u11, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
        "    \n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c11)\n",
        "    model = Model(inputs=[input_img], outputs=[outputs])\n",
        "    return model\n",
        "im_height = 224\n",
        "im_width = 224\n",
        "input_img = Input((im_height, im_width, 3), name = 'img')\n",
        "mainmod=get_unet(input_img)\n",
        "print(mainmod.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6fKeankb7cp",
        "outputId": "4a5419cc-cfa8-4284-8cea-837abf0a5831"
      },
      "source": [
        "import glob\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import tempfile\n",
        "import os\n",
        "from tensorflow.python.keras import optimizers\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "from skimage.util.dtype import convert\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "number_channel = 3\n",
        "im_width = 224\n",
        "im_height = 224\n",
        "border = 5\n",
        "\n",
        "def loading_data(path_img_train, path_mask_train):\n",
        "    #This function load training data (faces and mask, 128x128)\n",
        "    ids = next(os.walk(path_img_train))[2] # list of names all images in the given path\n",
        "    print(\"No. of train images = \", len(ids))\n",
        "    X = np.zeros((len(ids), im_height, im_width, number_channel), dtype=np.float32)\n",
        "    y = np.zeros((len(ids), im_height, im_width, 1), dtype=np.float32)\n",
        "    \n",
        "    for n in range(len(ids)):\n",
        "      img = load_img(path_img_train+'/'+ids[n], grayscale=False)\n",
        "      x_img = img_to_array(img)\n",
        "      x_img = resize(x_img, (im_height, im_width, number_channel), mode = 'constant', preserve_range = True)\n",
        "      # Load masks\n",
        "      mask = img_to_array(load_img(path_mask_train+'/'+ids[n], grayscale=True))\n",
        "      mask = resize(mask, (im_width, im_width, 1), mode = 'constant', preserve_range = True)\n",
        "      # Save images\n",
        "      X[n] = x_img/255.0\n",
        "      y[n] = mask/255.0\n",
        "        \n",
        "    print('Data Loaded')\n",
        "    \n",
        "    return X, y\n",
        "path_img_train = '/content/drive/MyDrive/HK5/XLA/HairSegmentation/dataset/faces_train'\n",
        "path_mask_train = '/content/drive/MyDrive/HK5/XLA/HairSegmentation/dataset/mask_train'\n",
        "\n",
        "X, y = loading_data(path_img_train, path_mask_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of train images =  5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
            "  warnings.warn('grayscale is deprecated. Please use '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYLr2CxqIYgt"
      },
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "input_img = Input((im_height, im_width, 3), name = 'img')\n",
        "\n",
        "model = get_unet(input_img)\n",
        "print(model.summary)\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "tensorboard= TensorBoard(log_dir='logs',\n",
        "                        histogram_freq=0,\n",
        "                        write_graph=True,\n",
        "                        write_images=True,\n",
        "                        write_grads=True,\n",
        "                        update_freq='epoch')\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=10, verbose=1),\n",
        "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1),\n",
        "    ModelCheckpoint('/content/drive/MyDrive/HK5/XLA/HairSegmentation/weight.h5', verbose=1, save_best_only=True, save_weights_only=True),\n",
        "    tensorboard\n",
        "]\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=5, epochs=50, callbacks=callbacks, validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyxp_CugcClv"
      },
      "source": [
        "MODEL_DIR = tempfile.gettempdir()\n",
        "version=5\n",
        "export_path = os.path.join(MODEL_DIR, str(version))\n",
        "print('export_path = {}\\n'.format(export_path))\n",
        "if os.path.isdir(export_path):\n",
        "    print('\\nAlready save a model, cleaning up\\n')\n",
        "    import shutil\n",
        "    shutil.rmtree(export_path)\n",
        "\n",
        "tf.saved_model.save(model, export_path)\n",
        "print('\\nSaved model:')\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_path)\n",
        "tflite_model = converter.convert()\n",
        "open(\"head_V4_mocel_.tflite\", \"wb\").write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}